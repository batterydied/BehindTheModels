{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d5aec47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from preprocessing import Preprocessor\n",
    "from asserts import asserts\n",
    "\n",
    "df = pd.read_csv('./data/student_habits_performance.csv')\n",
    "X = df.iloc[:, 1: -1]\n",
    "y = df.iloc[:, -1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5ecc454",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c42e799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocessor.preprocess(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f237558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TreeNode():\n",
    "    def __init__(self, left=None, right=None, feature_index=None, threshold=None, reduction_in_var=None, value=None):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.reduction_in_var = reduction_in_var\n",
    "        self.value = value  #for leaf nodes\n",
    "\n",
    "class DecisionTreeRegressor():\n",
    "    def __init__(self, min_samples_split=2, max_depth=5):\n",
    "        self.root = None\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def get_best_split(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        best_weighted_child_var = float('inf')\n",
    "        best_feature_index = None\n",
    "        best_threshold = None\n",
    "\n",
    "        for i in range(n_features):\n",
    "            sorted_col_indices = X[:, i].argsort()\n",
    "            sorted_col = X[:, i][sorted_col_indices]\n",
    "            sorted_y = y[sorted_col_indices]\n",
    "\n",
    "            for j in range(n_samples - 1):\n",
    "                threshold = (sorted_col[j] + sorted_col[j + 1]) / 2\n",
    "                left_y = sorted_y[sorted_col <= threshold]\n",
    "                right_y = sorted_y[sorted_col > threshold]\n",
    "\n",
    "                if len(left_y) < 2 or len(right_y) < 2:\n",
    "                    continue\n",
    "                    \n",
    "                weighted_left_var = np.var(left_y) * len(left_y) / n_samples\n",
    "                weighted_right_var = np.var(right_y) * len(right_y) / n_samples\n",
    "                weighted_child_var = weighted_left_var + weighted_right_var\n",
    "\n",
    "                if weighted_child_var < best_weighted_child_var:\n",
    "                    best_weighted_child_var = weighted_child_var\n",
    "                    best_feature_index = i\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        if best_feature_index is None:  #no valid split found\n",
    "            return None, None, None\n",
    "            \n",
    "        reduction_in_var = np.var(y) - best_weighted_child_var\n",
    "        return best_feature_index, best_threshold, reduction_in_var\n",
    "\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        n_samples = X.shape[0] if len(X.shape) > 1 else len(X)\n",
    "        \n",
    "        if n_samples < self.min_samples_split or depth >= self.max_depth:\n",
    "            return TreeNode(value=np.mean(y))\n",
    "        \n",
    "        feature_index, threshold, reduction_in_var = self.get_best_split(X, y)\n",
    "        if feature_index is None:  #no valid split found\n",
    "            return TreeNode(value=np.mean(y))\n",
    "\n",
    "        left_mask = X[:, feature_index] <= threshold\n",
    "        right_mask = ~left_mask\n",
    "        \n",
    "        X_left, y_left = X[left_mask], y[left_mask]\n",
    "        X_right, y_right = X[right_mask], y[right_mask]\n",
    "\n",
    "        left_child = self.build_tree(X_left, y_left, depth + 1)\n",
    "        right_child = self.build_tree(X_right, y_right, depth + 1)\n",
    "\n",
    "        return TreeNode(left_child, right_child, feature_index, threshold, reduction_in_var)\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        asserts(X_train, y_train)\n",
    "            \n",
    "        self.root = self.build_tree(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        if self.root is None:\n",
    "            raise ValueError(\"The tree is not fitted yet\")\n",
    "            \n",
    "        preds = np.zeros(len(X_test))\n",
    "        for i in range(len(X_test)):\n",
    "            node = self.root\n",
    "            while node.value is None:\n",
    "                if X_test[i, node.feature_index] <= node.threshold:\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    node = node.right\n",
    "            preds[i] = node.value\n",
    "        return preds\n",
    "\n",
    "    def score(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        rss = np.sum((y_test - y_pred) ** 2)\n",
    "        tss = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "        return 1 - (rss / tss) if tss != 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f3e16ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'OneHotEncoder' and 'OneHotEncoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m regressor \u001b[38;5;241m=\u001b[39m DecisionTreeRegressor()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mregressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m regressor\u001b[38;5;241m.\u001b[39mscore(X_test, y_test)\n",
      "Cell \u001b[0;32mIn[4], line 76\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X_train, y_train):\n\u001b[1;32m     74\u001b[0m     asserts(X_train, y_train)\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 58\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.build_tree\u001b[0;34m(self, X, y, depth)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples_split \u001b[38;5;129;01mor\u001b[39;00m depth \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TreeNode(value\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(y))\n\u001b[0;32m---> 58\u001b[0m feature_index, threshold, reduction_in_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m#no valid split found\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TreeNode(value\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(y))\n",
      "Cell \u001b[0;32mIn[4], line 25\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.get_best_split\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     22\u001b[0m best_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_features):\n\u001b[0;32m---> 25\u001b[0m     sorted_col_indices \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     sorted_col \u001b[38;5;241m=\u001b[39m X[:, i][sorted_col_indices]\n\u001b[1;32m     27\u001b[0m     sorted_y \u001b[38;5;241m=\u001b[39m y[sorted_col_indices]\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'OneHotEncoder' and 'OneHotEncoder'"
     ]
    }
   ],
   "source": [
    "regressor = DecisionTreeRegressor()\n",
    "regressor.fit(X_train, y_train)\n",
    "regressor.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90883b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brokestudent/Desktop/Projects/Behind the Models/.venv/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:246: UserWarning: Found unknown categories in columns [3] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input = [\n",
    "    [23, 'Female', 0.0, 1.2, 1.1, 'No', 85.0, 8.0, 'Fair', 6, 'Master', 'Average', 8, 'Yes'],\n",
    "    [20, 'Female', 6.9, 2.8, 2.3, 'No', 97.3, 4.6, 'Good', 6, 'High School', 'Average', 8, 'No'],\n",
    "    [21, 'Male', 1.4, 3.1, 1.3, 'No', 94.8, 8.0, 'Poor', 1, 'High School', 'Poor', 1, 'No'],\n",
    "    [23, 'Female', 1.0, 3.9, 1.0, 'No', 71.0, 9.2, 'Poor', 4, 'Master', 'Good', 1, 'Yes'],\n",
    "    [19, 'Female', 5.0, 4.4, 0.5, 'No', 90.9, 4.9, 'Fair', 3, 'Master', 'Good', 1, 'No'],\n",
    "    [24, 'Male', 7.2, 1.3, 0.0, 'No', 82.9, 7.4, 'Fair', 1, 'Master', 'Average', 4, 'No'],\n",
    "    [21, 'Female', 5.6, 1.5, 1.4, 'Yes', 85.8, 6.5, 'Good', 2, 'Master', 'Poor', 4, 'No'],\n",
    "    [21, 'Female', 4.3, 1.0, 2.0, 'Yes', 77.7, 4.6, 'Fair', 0, 'Bachelor', 'Average', 8, 'No'],\n",
    "    [23, 'Female', 4.4, 2.2, 1.7, 'No', 100.0, 7.1, 'Good', 3, 'Bachelor', 'Good', 1, 'No'],\n",
    "    [18, 'Female', 4.8, 3.1, 1.3, 'No', 95.4, 7.5, 'Good', 5, 'Bachelor', 'Good', 10, 'Yes'],\n",
    "    [19, 'Female', 4.6, 3.7, 0.8, 'No', 77.6, 5.8, 'Fair', 1, 'None', 'Good', 3, 'No'],\n",
    "]\n",
    "\n",
    "input = preprocessor.transform_input(input)\n",
    "my_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529212c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7519831168449562"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, regressor.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec6dbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7000335660050903"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "dtr = sklearn.tree.DecisionTreeRegressor()\n",
    "dtr.fit(X_train, y_train)\n",
    "dtr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f4fc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_pred = dtr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0220675f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions within 15 distance: 210\n",
      "0.84\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for pred1, pred2 in zip(my_pred, sklearn_pred):\n",
    "    if abs(pred1 - pred2) <= 10:\n",
    "        count += 1\n",
    "print(f\"Number of predictions within 10 distance: {count}\")\n",
    "print(count/len(my_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
